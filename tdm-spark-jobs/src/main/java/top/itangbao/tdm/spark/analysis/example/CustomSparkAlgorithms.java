package top.itangbao.tdm.spark.analysis.example;

import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.broadcast.Broadcast;
import org.apache.spark.ml.Estimator;
import org.apache.spark.ml.Model;
import org.apache.spark.ml.param.Param;
import org.apache.spark.ml.param.ParamMap;
import org.apache.spark.ml.util.DefaultParamsWritable;
import org.apache.spark.ml.util.Identifiable;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.RowFactory;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;
import scala.Option;

import java.util.Arrays;
import java.util.List;

// å¯¼å…¥ Spark SQL é™æ€å‡½æ•°ï¼Œå¦‚ col(), min(), max()
import static org.apache.spark.sql.functions.*;

/**
 * æ¼”ç¤ºåœ¨ Spark ä¸­å®ç°è‡ªå®šä¹‰ç®—æ³•çš„ä¸‰ç§æ–¹å¼
 *
 * è¿™ä¸ªç®—æ³•çš„é€»è¾‘æ˜¯ï¼š
 *
 * æ‰¾åˆ°æŸä¸€åˆ—çš„æœ€å°å€¼ (min) å’Œæœ€å¤§å€¼ (max)ã€‚
 *
 * å¯¹è¯¥åˆ—ä¸­çš„æ¯ä¸€ä¸ªå€¼ xï¼Œåº”ç”¨å…¬å¼ scaled_x = (x - min) / (max - min)ï¼Œå°†æ‰€æœ‰å€¼ç¼©æ”¾åˆ° [0, 1] åŒºé—´ã€‚
 *
 * ----------------------------------------------------------------------------------------------------
 * åœ¨ Spark ä¸­ï¼Œâ€œå¸¸ç”¨ç®—æ³•â€ä¸»è¦å¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªå¤§å®¶æ—ï¼Œå…¶ä¸­æœºå™¨å­¦ä¹ åº“ (MLlib) æ˜¯é‡ä¸­ä¹‹é‡ã€‚
 *
 * 1. ğŸ¤– æœºå™¨å­¦ä¹  (MLlib) - Spark çš„çœ‹å®¶æœ¬é¢†
 * è¿™æ˜¯ Spark ä¸­æœ€å¤§ã€æœ€å¸¸ç”¨çš„ç®—æ³•åº“ã€‚å®ƒå‡ ä¹æ¶µç›–äº†æ‰€æœ‰ä¸»æµçš„ç›‘ç£å’Œéç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚
 *
 * A. åˆ†ç±» (Classification)
 * å½“ä½ çš„ç›®æ ‡æ˜¯é¢„æµ‹ä¸€ä¸ªç±»åˆ«æ—¶ï¼ˆæ¯”å¦‚ï¼šæ˜¯ä¸æ˜¯åƒåœ¾é‚®ä»¶ï¼Ÿå®¢æˆ·æ˜¯å¦ä¼šæµå¤±ï¼Ÿï¼‰ã€‚
 *
 * é€»è¾‘å›å½’ (Logistic Regression): æœ€åŸºç¡€ä¹Ÿæ˜¯æœ€å¸¸ç”¨çš„äºŒåˆ†ç±»ç®—æ³•ï¼Œé€Ÿåº¦å¿«ï¼Œæ˜“äºè§£é‡Šã€‚
 *
 * å†³ç­–æ ‘ (Decision Trees): éå¸¸ç›´è§‚ï¼Œåƒä¸€ä¸ªæµç¨‹å›¾ï¼Œå¯è§£é‡Šæ€§å¼ºã€‚
 *
 * éšæœºæ£®æ— (Random Forest): â€œæ£®æ—â€ç”±å¾ˆå¤šâ€œå†³ç­–æ ‘â€ç»„æˆã€‚å®ƒé€šè¿‡æŠ•ç¥¨æ¥æé«˜å‡†ç¡®æ€§ï¼Œæ˜¯ç›®å‰æœ€å¥½ç”¨çš„â€œå¼€ç®±å³ç”¨â€ç®—æ³•ä¹‹ä¸€ã€‚
 *
 * GBTs (Gradient-Boosted Trees, æ¢¯åº¦æå‡æ ‘): å¦ä¸€ç§åŸºäºæ ‘çš„å¼ºå¤§ç®—æ³•ï¼Œé€šå¸¸åœ¨å„ç§æ•°æ®ç«èµ›ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå‡†ç¡®ç‡æé«˜ï¼Œä½†è®­ç»ƒè¾ƒæ…¢ã€‚
 *
 * æœ´ç´ è´å¶æ–¯ (Naive Bayes): å°¤å…¶æ“…é•¿æ–‡æœ¬åˆ†ç±»ï¼ˆæ¯”å¦‚åƒåœ¾é‚®ä»¶è¿‡æ»¤ï¼‰ã€‚
 *
 * B. å›å½’ (Regression)
 * å½“ä½ çš„ç›®æ ‡æ˜¯é¢„æµ‹ä¸€ä¸ªè¿ç»­çš„æ•°å€¼æ—¶ï¼ˆæ¯”å¦‚ï¼šé¢„æµ‹æˆ¿ä»·ã€é¢„æµ‹é”€é‡ï¼‰ã€‚
 *
 * çº¿æ€§å›å½’ (Linear Regression): æœ€åŸºç¡€çš„å›å½’ç®—æ³•ï¼Œç”¨äºå¯»æ‰¾å˜é‡é—´çš„çº¿æ€§å…³ç³»ã€‚
 *
 * å†³ç­–æ ‘ / éšæœºæ£®æ— / GBTs: æ²¡é”™ï¼Œè¿™äº›åŸºäºæ ‘çš„æ¨¡å‹åŒæ ·å¯ä»¥ç”¨äºå›å½’ä»»åŠ¡ï¼Œè¡¨ç°ä¹Ÿéå¸¸å‡ºè‰²ã€‚
 *
 * å¹¿ä¹‰çº¿æ€§å›å½’ (Generalized Linear Regression, GLR): çº¿æ€§å›å½’çš„æ‰©å±•ï¼Œå…è®¸ä½ å¤„ç†ä¸åŒç±»å‹çš„æ•°æ®åˆ†å¸ƒï¼ˆæ¯”å¦‚æ³Šæ¾å›å½’ç”¨äºè®¡æ•°æ•°æ®ï¼‰ã€‚
 *
 * C. èšç±» (Clustering)
 * å½“ä½ çš„æ•°æ®æ²¡æœ‰â€œç­”æ¡ˆâ€ï¼ˆæ ‡ç­¾ï¼‰æ—¶ï¼Œä½ æƒ³è®©ç®—æ³•è‡ªåŠ¨æ‰¾å‡ºæ•°æ®ä¸­çš„â€œç¾¤ç»„â€ã€‚
 *
 * K-Means (Kå‡å€¼): æœ€è‘—åã€æœ€ç®€å•çš„èšç±»ç®—æ³•ã€‚ä½ éœ€è¦é¢„å…ˆæŒ‡å®šè¦æ‰¾å‡ºçš„ç¾¤ç»„æ•°é‡ (K)ã€‚
 *
 * GMM (Gaussian Mixture Model, é«˜æ–¯æ··åˆæ¨¡å‹): K-Means çš„å‡çº§ç‰ˆã€‚å®ƒæ›´çµæ´»ï¼Œå…è®¸æ•°æ®ç‚¹â€œè½¯æ€§â€åœ°å±äºå¤šä¸ªèšç±»ã€‚
 *
 * LDA (Latent Dirichlet Allocation): ä¸»è¦ç”¨äºä¸»é¢˜æ¨¡å‹ (Topic Modeling)ã€‚å®ƒå¯ä»¥ä»ä¸€å †æ–‡æ¡£ä¸­è‡ªåŠ¨å‘ç°â€œä¸»é¢˜â€ï¼ˆæ¯”å¦‚æŠŠæ–°é—»è‡ªåŠ¨åˆ†ä¸ºâ€œä½“è‚²â€ã€â€œè´¢ç»â€ã€â€œç§‘æŠ€â€ï¼‰ã€‚
 *
 * D. æ¨è (Recommendation)
 * ç”¨äºâ€œçŒœä½ å–œæ¬¢â€çš„åœºæ™¯ï¼Œæ¯”å¦‚ç”µå•†ã€è§†é¢‘ç½‘ç«™ã€‚
 *
 * ALS (Alternating Least Squares, äº¤æ›¿æœ€å°äºŒä¹˜æ³•): è¿™æ˜¯ Spark MLlib ä¸­ç”¨äºæ„å»ºååŒè¿‡æ»¤ (Collaborative Filtering) æ¨èç³»ç»Ÿçš„æ ¸å¿ƒç®—æ³•ã€‚
 *
 * 2. ğŸ”¬ ç‰¹å¾å·¥ç¨‹ä¸ç»Ÿè®¡ (Feature & Statistics)
 * è¿™ä¸€ç±»å’Œä½ çš„ FFT å¾ˆåƒï¼Œå®ƒä»¬æœ¬èº«ä¸æ˜¯â€œé¢„æµ‹æ¨¡å‹â€ï¼Œè€Œæ˜¯**â€œå‡†å¤‡æ•°æ®â€å’Œâ€œç†è§£æ•°æ®â€**çš„ç®—æ³•ã€‚
 *
 * PCA (Principal Component Analysis, ä¸»æˆåˆ†åˆ†æ): éå¸¸å¼ºå¤§çš„é™ç»´ç®—æ³•ã€‚å½“ä½ çš„æ•°æ®æœ‰å‡ ç™¾ä¸Šåƒåˆ—ï¼ˆç‰¹å¾ï¼‰æ—¶ï¼ŒPCA å¯ä»¥å¸®ä½ æŠŠå®ƒä»¬å‹ç¼©åˆ°å°‘æ•°å‡ ä¸ªâ€œä¸»æˆåˆ†â€ä¸Šï¼ŒåŒæ—¶ä¿ç•™å¤§éƒ¨åˆ†ä¿¡æ¯ã€‚
 *
 * TF-IDF (Term Frequency-Inverse Document Frequency): æ–‡æœ¬åˆ†æçš„åŸºçŸ³ã€‚ç”¨äºè¡¡é‡ä¸€ä¸ªè¯åœ¨æ–‡æ¡£ä¸­çš„é‡è¦æ€§ã€‚
 *
 * Word2Vec: ä¸€ç§â€œè¯åµŒå…¥â€ç®—æ³•ï¼Œå®ƒèƒ½æŠŠå•è¯è½¬æ¢æˆæ•°å­¦å‘é‡ï¼Œè®©æœºå™¨ç†è§£è¯ä¸è¯ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ï¼ˆæ¯”å¦‚ "å›½ç‹" - "ç”·äºº" + "å¥³äºº" â‰ˆ "ç‹å"ï¼‰ã€‚
 *
 * ç›¸å…³æ€§åˆ†æ (Correlation): è®¡ç®—ä¸åŒç‰¹å¾ä¹‹é—´çš„çš®å°”é€Š (Pearson) æˆ–æ–¯çš®å°”æ›¼ (Spearman) ç›¸å…³ç³»æ•°ã€‚è¿™æ˜¯æ•°æ®æ¢ç´¢çš„ç¬¬ä¸€æ­¥ï¼Œå¸®ä½ ç†è§£å“ªäº›å˜é‡æ˜¯ç›¸å…³çš„ã€‚
 *
 * 3. ğŸ•¸ï¸ å›¾è®¡ç®— (GraphX)
 * å¦‚æœä½ çš„æ•°æ®æ˜¯â€œç½‘ç»œâ€ç»“æ„ï¼ˆæ¯”å¦‚ç¤¾äº¤ç½‘ç»œçš„ç”¨æˆ·å…³ç³»ã€ç‰©æµçš„èŠ‚ç‚¹ç½‘ç»œï¼‰ï¼ŒSpark æœ‰ä¸€ä¸ªä¸“é—¨çš„åº“ GraphXã€‚
 *
 * PageRank: è°·æ­Œæ—©æœŸçš„æ ¸å¿ƒç®—æ³•ï¼Œç”¨äºè¯„ä¼°ç½‘é¡µçš„é‡è¦æ€§ï¼ˆèŠ‚ç‚¹çš„é‡è¦æ€§ï¼‰ã€‚
 *
 * Connected Components (è¿é€šåˆ†é‡): æ‰¾å‡ºç½‘ç»œä¸­ç›¸äº’è¿æ¥çš„â€œç¾¤ç»„â€ã€‚
 *
 * Triangle Counting (ä¸‰è§’å½¢è®¡æ•°): åˆ†æç½‘ç»œçš„ç´§å¯†ç¨‹åº¦ï¼Œå¸¸ç”¨äºç¤¾åŒºå‘ç°ã€‚
 *
 * 4. ğŸ›’ æ¨¡å¼æŒ–æ˜ (Pattern Mining)
 * FP-Growth: ç”¨äºé¢‘ç¹é¡¹é›†æŒ–æ˜ã€‚æœ€ç»å…¸çš„ä¾‹å­æ˜¯â€œå•¤é…’ä¸å°¿å¸ƒâ€çš„å…³è”è§„åˆ™ (Market Basket Analysis)ï¼Œæ‰¾å‡ºå“ªäº›ä¸œè¥¿ç»å¸¸è¢«ä¸€èµ·è´­ä¹°ã€‚
 */
public class CustomSparkAlgorithms {

    /**
     * æ–¹å¼ä¸€ï¼šç»„åˆ DataFrame API
     * ç†å¿µï¼šä½¿ç”¨é«˜çº§å‡½æ•°å¼ APIã€‚ä»£ç ç®€æ´ã€å¯è¯»æ€§å¼ºï¼Œå¹¶èƒ½äº«å— Catalyst ä¼˜åŒ–ã€‚
     * é€‚ç”¨ï¼šç»å¤§å¤šæ•°ç‰¹å¾å·¥ç¨‹å’Œä¸šåŠ¡é€»è¾‘ã€‚
     */
    public static class Method1_DataFrame {

        public static Dataset<Row> scale(Dataset<Row> df, String inputCol, String outputCol) {
            System.out.println("--- è¿è¡Œæ–¹å¼ä¸€ï¼šDataFrame API ---");

            // 1. ä½¿ç”¨ agg() ä¸€æ¬¡æ€§è®¡ç®— min å’Œ max
            // .first() ä¼šè¿”å›ä¸€ä¸ª Rowï¼Œæ ¼å¼ä¸º [min_val, max_val]
            Row minMaxRow = df.agg(min(inputCol), max(inputCol)).first();
            double minVal = minMaxRow.getDouble(0);
            double maxVal = minMaxRow.getDouble(1);
            double range = maxVal - minVal;

            // æ£€æŸ¥é™¤é›¶é”™è¯¯
            if (range == 0.0) {
                // å¦‚æœ max == minï¼Œæ‰€æœ‰å€¼éƒ½ä¸€æ ·ï¼Œå¯ä»¥å°†å®ƒä»¬ç¼©æ”¾ä¸º 0 æˆ– 0.5
                return df.withColumn(outputCol, lit(0.5));
            }

            // 2. ä½¿ç”¨ withColumn() å’Œåˆ—æ“ä½œæ¥åº”ç”¨å…¬å¼
            Dataset<Row> scaledDf = df.withColumn(outputCol,
                    (col(inputCol).minus(minVal)).divide(range)
            );

            return scaledDf;
        }
    }


    /**
     * æ–¹å¼äºŒï¼šå®ç° MLlib Pipeline æ¥å£
     * ç†å¿µï¼šéµå¾ª Spark MLlib çš„æ ‡å‡†ï¼Œåˆ›å»ºå¯é‡ç”¨çš„ã€å¯åŠ å…¥ Pipeline çš„ç»„ä»¶ã€‚
     * é€‚ç”¨ï¼šå®ç°æ ‡å‡†çš„ã€å¯é‡ç”¨çš„æœºå™¨å­¦ä¹ æ­¥éª¤ã€‚
     *
     * æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ª Estimator (ä¼°è®¡å™¨)ï¼Œå®ƒ .fit() æ•°æ®æ¥å­¦ä¹  min/maxï¼Œ
     * ç„¶åè¿”å›ä¸€ä¸ª Transformer (è½¬æ¢å™¨) æˆ– Model (æ¨¡å‹) æ¥ .transform() æ•°æ®ã€‚
     */
    public static class Method2_MLlib {

        // --- ä¼°è®¡å™¨ (Estimator)ï¼šç”¨äº "å­¦ä¹ " ---
        // å®ƒè´Ÿè´£ .fit()ï¼Œè®¡ç®— min å’Œ max
        public static class MyMinMaxScaler extends Estimator<MyMinMaxScalerModel> implements DefaultParamsWritable {

            private final String uid;
            private final Param<String> inputCol;
            private final Param<String> outputCol;

            public MyMinMaxScaler(String uid) {
                this.uid = uid;
                this.inputCol = new Param<>(this, "inputCol", "The input column");
                this.outputCol = new Param<>(this, "outputCol", "The output column");
            }
            public MyMinMaxScaler() { this(Identifiable.randomUID("myMinMaxScaler")); }

            // Setters
            public MyMinMaxScaler setInputCol(String value) { set(inputCol, value); return this; }
            public MyMinMaxScaler setOutputCol(String value) { set(outputCol, value); return this; }

            // Getters
            public Option<String> getInputCol() { return get(inputCol); }
            public Option<String> getOutputCol() { return get(outputCol); }

            @Override
            public MyMinMaxScalerModel fit(Dataset<?> dataset) {
                // 1. ä»è®­ç»ƒæ•°æ®ä¸­ "å­¦ä¹ " min å’Œ max
                String colName = getInputCol().get();
                Row minMaxRow = dataset.agg(min(colName), max(colName)).first();
                double minVal = minMaxRow.getDouble(0);
                double maxVal = minMaxRow.getDouble(1);

                // 2. è¿”å›ä¸€ä¸ª "æ¨¡å‹" (Transformer)ï¼Œè¯¥æ¨¡å‹å­˜å‚¨äº†å­¦ä¹ åˆ°çš„ min/max
                return new MyMinMaxScalerModel(this.uid, minVal, maxVal)
                        .setInputCol(colName)
                        .setOutputCol(getOutputCol().get());
            }

            @Override
            public StructType transformSchema(StructType schema) {
                // æ£€æŸ¥è¾“å…¥åˆ—æ˜¯å¦å­˜åœ¨ä¸”ä¸ºæ•°å€¼ç±»å‹
                String inCol = getInputCol().get();
                if (!schema.simpleString().contains(inCol)) {
                    throw new IllegalArgumentException("Input column " + inCol + " does not exist.");
                }
                // æ·»åŠ è¾“å‡ºåˆ—
                return schema.add(getOutputCol().get(), DataTypes.DoubleType, false);
            }

            @Override
            public MyMinMaxScaler copy(ParamMap extra) {
                return defaultCopy(extra);
            }

            @Override
            public String uid() { return this.uid; }
        }

        // --- æ¨¡å‹ (Model / Transformer)ï¼šç”¨äº "è½¬æ¢" ---
        // å®ƒå­˜å‚¨ min/max å¹¶åº”ç”¨ .transform()
        public static class MyMinMaxScalerModel extends Model<MyMinMaxScalerModel> implements DefaultParamsWritable {

            private final String uid;
            private final double originalMin;
            private final double originalMax;
            private final Param<String> inputCol;
            private final Param<String> outputCol;

            public MyMinMaxScalerModel(String uid, double min, double max) {
                this.uid = uid;
                this.originalMin = min;
                this.originalMax = max;
                this.inputCol = new Param<>(this, "inputCol", "The input column");
                this.outputCol = new Param<>(this, "outputCol", "The output column");
            }

            // Setters
            public MyMinMaxScalerModel setInputCol(String value) { set(inputCol, value); return this; }
            public MyMinMaxScalerModel setOutputCol(String value) { set(outputCol, value); return this; }

            @Override
            public Dataset<Row> transform(Dataset<?> dataset) {
                double range = originalMax - originalMin;
                if (range == 0.0) {
                    return dataset.withColumn(get(outputCol).get(), lit(0.5));
                }

                return dataset.withColumn(get(outputCol).get(),
                        (col(get(inputCol).get()).minus(originalMin)).divide(range)
                );
            }

            @Override
            public StructType transformSchema(StructType schema) {
                return schema.add(get(outputCol).get(), DataTypes.DoubleType, false);
            }

            @Override
            public MyMinMaxScalerModel copy(ParamMap extra) {
                return defaultCopy(extra);
            }
            @Override
            public String uid() { return this.uid; }
        }
    }


    /**
     * æ–¹å¼ä¸‰ï¼šä½¿ç”¨ RDD API
     * ç†å¿µï¼šæœ€åº•å±‚çš„ã€æ‰‹åŠ¨çš„ã€æŒ‡ä»¤å¼çš„æ§åˆ¶ã€‚ä½ å¿…é¡»è‡ªå·±ç®¡ç†æ‰€æœ‰äº‹æƒ…ã€‚
     * é€‚ç”¨ï¼šæ— æ³•ç”¨ DataFrame è¡¨è¾¾çš„ã€å¤æ‚çš„è¿­ä»£ç®—æ³•æˆ–éœ€è¦ç²¾ç»†åˆ†åŒºæ§åˆ¶çš„ç®—æ³•ã€‚
     */
    public static class Method3_RDD {

        public static Dataset<Row> scale(Dataset<Row> df, String inputCol) {
            System.out.println("--- è¿è¡Œæ–¹å¼ä¸‰ï¼šRDD API ---");

            JavaSparkContext jsc = new JavaSparkContext(df.sparkSession().sparkContext());
            int colIndex = df.schema().fieldIndex(inputCol);

            // 1. å°† DataFrame è½¬æ¢ä¸º RDD
            JavaRDD<Row> rdd = df.javaRDD();

            // 2. åœ¨ä¸€æ¬¡ä¼ é€’ä¸­è®¡ç®— min å’Œ max
            // (è¿™æ˜¯æ¯”å…ˆ mapToDouble å†è°ƒç”¨ .min() å’Œ .max() æ›´é«˜æ•ˆçš„ RDD æ–¹å¼)
            // (a) å°† RDD æ˜ å°„ä¸º (value, value)
            // (b) ä½¿ç”¨ reduce æ‰¾åˆ° (min, max)
            scala.Tuple2<Double, Double> minMax = rdd.map(row -> {
                double val = row.getDouble(colIndex);
                return new scala.Tuple2<>(val, val);
            }).reduce((tuple1, tuple2) -> {
                double min = Math.min(tuple1._1, tuple2._1);
                double max = Math.max(tuple1._2, tuple2._2);
                return new scala.Tuple2<>(min, max);
            });

            double minVal = minMax._1;
            double maxVal = minMax._2;
            double range = maxVal - minVal;

            // 3. å°† min å’Œ range å¹¿æ’­ (Broadcast) åˆ°æ‰€æœ‰ Executor
            // è¿™æ˜¯ RDD ç¼–ç¨‹çš„æœ€ä½³å®è·µï¼Œé¿å…åœ¨ map é—­åŒ…ä¸­åºåˆ—åŒ–æ•´ä¸ªä»»åŠ¡
            Broadcast<Double> bMin = jsc.broadcast(minVal);
            Broadcast<Double> bRange = jsc.broadcast(range);

            // 4. å†æ¬¡éå† RDD (map) ä»¥åº”ç”¨ç¼©æ”¾
            JavaRDD<Row> scaledRDD = rdd.map(row -> {
                double originalVal = row.getDouble(colIndex);
                double scaledVal = 0.5; // é»˜è®¤å€¼ (å¦‚æœ range == 0)

                double bRangeVal = bRange.value();
                if (bRangeVal != 0.0) {
                    scaledVal = (originalVal - bMin.value()) / bRangeVal;
                }
                // æ³¨æ„ï¼šä½ éœ€è¦æ‰‹åŠ¨é‡å»º Row
                return RowFactory.create(originalVal, scaledVal);
            });

            // 5. å°† RDD è½¬æ¢å› DataFrameï¼Œä½ éœ€è¦æ‰‹åŠ¨å®šä¹‰ Schema
            StructType newSchema = new StructType(new StructField[]{
                    new StructField(inputCol, DataTypes.DoubleType, false, null),
                    new StructField("scaled", DataTypes.DoubleType, false, null)
            });

            return df.sparkSession().createDataFrame(scaledRDD, newSchema);
        }
    }


    /**
     * -----------------------------------------------------------------
     * è¿è¡Œæ‰€æœ‰ç¤ºä¾‹çš„ Main æ–¹æ³•
     * -----------------------------------------------------------------
     */
    public static void main(String[] args) {

        SparkSession spark = SparkSession.builder()
                .appName("CustomSparkAlgorithmsDemo")
                .master("local[*]")
                .getOrCreate();

        // 1. å‡†å¤‡ç¤ºä¾‹æ•°æ®
        List<Row> data = Arrays.asList(
                RowFactory.create(1.0),
                RowFactory.create(2.0),
                RowFactory.create(3.0),
                RowFactory.create(4.0),
                RowFactory.create(5.0)
        );
        StructType schema = new StructType(new StructField[]{
                new StructField("feature", DataTypes.DoubleType, false, null)
        });
        Dataset<Row> df = spark.createDataFrame(data, schema);
        System.out.println("åŸå§‹æ•°æ®:");
        df.show();

        // --- æ‰§è¡Œæ–¹å¼ä¸€ ---
        Dataset<Row> df1 = Method1_DataFrame.scale(df, "feature", "scaled");
        df1.show();

        // --- æ‰§è¡Œæ–¹å¼äºŒ ---
        System.out.println("--- è¿è¡Œæ–¹å¼äºŒï¼šMLlib API ---");
        Method2_MLlib.MyMinMaxScaler scaler = new Method2_MLlib.MyMinMaxScaler()
                .setInputCol("feature")
                .setOutputCol("scaled");

        // è®­ç»ƒ (fit)ï¼šè®¡ç®— min/max
        Method2_MLlib.MyMinMaxScalerModel model = scaler.fit(df);

        // è½¬æ¢ (transform)ï¼šåº”ç”¨å…¬å¼
        Dataset<Row> df2 = model.transform(df);
        df2.show();

        // --- æ‰§è¡Œæ–¹å¼ä¸‰ ---
        Dataset<Row> df3 = Method3_RDD.scale(df, "feature");
        df3.show();

        spark.stop();
    }
}
